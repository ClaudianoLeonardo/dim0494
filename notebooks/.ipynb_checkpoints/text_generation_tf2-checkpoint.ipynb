{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0a2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde89fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file ='shakespeare.txt'\n",
    "if os.path.exists(path_to_file) is False:\n",
    "    path_to_file =tf.keras.utils.get_file(\n",
    "        'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bcbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read text\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af63cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9c0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17db3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:51:39.665805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.665992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.670326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.670470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.670602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.670731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.671356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 23:51:39.755031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.755179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.755296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.755403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.755516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:39.755622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.260835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.260995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.261109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.261220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.261328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.261435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10540 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:01:00.0, compute capability: 7.0\n",
      "2022-11-07 23:51:40.261697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 23:51:40.261803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6677 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the strings to a numerical representation.\n",
    "# The tf.keras.layers.StringLookup layer can convert each character into a numeric ID.\n",
    "# It just needs the text to be split into tokens first.\n",
    "\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bf8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters -> numbers (layer)\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93875ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts from tokens to character IDs:\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfcb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers -> ids (layer)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de844b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numbers -> characters\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebc65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join characters back into text (layer)\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7369c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numbers into text\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f55e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text -> stream of indices (numbers)\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f71ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02cf50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d610c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the textual sequences to be used\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d51ca34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379d5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7339140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training you'll need a dataset of (input, label) pairs.\n",
    "# Where input and label are sequences.\n",
    "# At each time step the input is the current character and the label is the next character.\n",
    "# Here's a function that takes a sequence as input, duplicates, and shifts it\n",
    "# to align the input and label for each timestep:\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e75c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b674d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "281f7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38cfa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3543afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649f87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "193e2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dfe32b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:52:50.076836: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# trying the model\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a74c531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "159a837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get actual predictions from the model you need to sample from the output distribution,\n",
    "#to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940e9460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 64, 48,  8, 22, 52, 22,  6,  7, 65, 62, 47,  6, 46, 15, 38, 46,\n",
       "       50, 11,  3, 22, 24, 15,  2, 64, 37, 38, 26, 55,  5, 21, 10, 39, 55,\n",
       "       21, 56, 65, 19,  9, 21, 45, 16, 48, 16, 20, 36, 59, 13,  1, 56, 61,\n",
       "       41,  9, 26, 51, 26,  2, 54, 16, 59, 31, 61, 60, 56, 49, 27, 57, 36,\n",
       "       33, 40, 16, 27,  2,  1, 36, 30, 37, 18, 10, 39, 21, 41, 53, 19,  6,\n",
       "       34, 27, 62, 62, 24, 17,  2, 30, 40,  6, 60, 26, 41, 52, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2e3da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'ight feel your love\\nThan my unpleased eye see your courtesy.\\nUp, cousin, up; your heart is up, I kno'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"Iyi-ImI',zwh'gBYgk:!IKB yXYMp&H3ZpHqzF.HfCiCGWt?\\nqvb.MlM oCtRvuqjNrWTaCN \\nWQXE3ZHbnF'UNwwKD Qa'uMbmN\"\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted by this untrained model:\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f66a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e912f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.189377, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# cost (untrained)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e59084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.98166"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba48600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1272ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d67753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e9e1fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 6s 25ms/step - loss: 2.7244\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 2.0000\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.7160\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.5520\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.4517\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.3835\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 1.3296\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.2859\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.2445\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 5s 25ms/step - loss: 1.2050\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 5s 26ms/step - loss: 1.1660\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 5s 27ms/step - loss: 1.1251\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 5s 27ms/step - loss: 1.0820\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 5s 28ms/step - loss: 1.0372\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 5s 28ms/step - loss: 0.9895\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 5s 28ms/step - loss: 0.9389\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 5s 28ms/step - loss: 0.8872\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 5s 28ms/step - loss: 0.8356\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 5s 27ms/step - loss: 0.7843\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 5s 27ms/step - loss: 0.7366\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04baa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation model\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09448bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d49e5103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Villain, stir! strike! do you know this countryman:\n",
      "The swift are and of thyself none at the clusch,\n",
      "What treachery face thrusts no harm such sworn,\n",
      "Which, though thou wilt obedien: this time that\n",
      "As after reward, or that he knews,\n",
      "You worldle steel, thus\n",
      "Glood'd unto justice footing to the tenter as\n",
      "we hear himself.' but, then, I do not\n",
      "Too hot to pardon Rome! humbly thou\n",
      "affript the crown.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "What would your genitate here?\n",
      "\n",
      "O God,\n",
      "What news, invoint, my tent?\n",
      "\n",
      "CATESBY:\n",
      "Now, Words, report is cold of his the covertood; your\n",
      "hongiers. You are a Roman;\n",
      "For sleep my slave: where he should hear him stand:\n",
      "This is a great deputy, but the wish of breath,\n",
      "That he hath done met, though a cup of Rosaline.\n",
      "\n",
      "RIVERS:\n",
      "And therefore be rilier, another cause.\n",
      "\n",
      "GLOUCESTER:\n",
      "What think so fars? Is the beauty o'er-heads\n",
      "our foe, and beat your parts, York and Edward's love,\n",
      "Immedies 'gainst the throne a heavenly hate;\n",
      "And show no warrant changing ill:\n",
      "Is it thou corclivites me in his stake  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.932978630065918\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c57ee649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe which my sues, you are like gozer Marcius,\\nEre we were all unhappier than you, against his strong:\\nIre parted to reply, at the lie I'll lady\\nAnd still to purpose and virtuous wrongs\\nAnd downright by thine. And as I turn:\\nTo hon, I, believe not this beful mine.\\n\\nRICHMOND:\\nGod and hang'd children not the chirace of ourselves:\\nWhy, then, as I do priso leave myself to\\ndance.\\n\\nPAGE:\\nThis proud infect what dangers or others, parts,\\nYet that this mould be to stand, and be seen to be;\\nFor me, that he be lost in heaven;\\nThe comfort, do not say he is.\\n\\nGLOUCESTER:\\nWhat, would you have meal due? when you have must one\\nand honour to my clothes till now\\nIn Margaret's sacred household room exile\\nThat down while heaven confine you,\\nShall satisfied that deadly was but a piteous crown,\\nAnd never toward the nornes. The neal of tears!\\n\\nWARWICK:\\nThen this, good friend! when every story reado\\nI have spent my knot in blood and pity\\nSomewhing your bones that Ruts and rotteness\\nTo this conventent that th\"\n",
      " b\"ROMEO:\\nFor being one another, boy.\\n\\nClown:\\nHold, hold!\\nI can be a hand is thence?\\n\\nMENENIUS:\\nI would you have made no friend to deep upon thee.\\nThe statues of a complessiob like me.\\n\\nPETRUCHIO:\\nI prithee, my lord. What says my master!\\n\\nDERBY:\\nMy noble Plantagenet,--\\n\\nDUCHESS OF YORK:\\n\\nHENRY BOLINGBROKE:\\nGo have your hands;\\nFor I have muse accuse thee with the air.\\n\\nBIONDELLO:\\nHe does got in Peace, sir, he has done now,\\nNot up a dream of heaven? the case as you,\\nAs I have eyes conceived with sorrow's middless death\\nAnd every meaner sense outward wothing clouds,\\nAs if this hurly I in great apollo.\\n\\nROMEO:\\nHe doth it in the pempless of the air!\\n\\nJULIET:\\nNo, no: but having, march on join of justice brain warriors,\\nWith given such corse, as woman's faces cheers forgot,\\nBut, for, in towards our Rome hath resisted my rich,\\nThere any of it: it may be absent,\\nYou shakest in some sick heavens to this If.\\nIn my trumper's eyes ungovy 'grazen;\\nThis is the executioner.\\n\\nGLOUCESTER:\\nWe thank your grace. I'\"\n",
      " b\"ROMEO:\\nVillain, I see, which needs me weep,\\nSo they at Pearable gentleman:\\nDraws thee for my maner? what a torment I do?\\n\\nDUKE VINCENTIO:\\nIf it peer me along.\\n\\nProvost:\\nI do not know what strict hardly given to her:\\nThings traitor, Kate, and are there,\\nMay be against in vain.\\nThou talk'd, a husband to command;\\nPoor stucing surse, but with usurper revenge\\nBut newly in the high of war,\\nAs yonder looks and honour from hen,\\nDo me return to live.\\n\\nGREMIO:\\nBend thy cold and daughter, your name, the son,\\nYour full of death, my books and let your mejesh\\nTo hear their cruel strength and fine shall visit\\nNothing that death he marks that placed his charge;\\nAnd she came untimely from to death,\\n'Tas I possess'd, in Warwick.\\n\\nTRANIO:\\nThe most struck him.\\n\\nSICINIUS:\\nHelp not pale.\\n\\nGRUMIO:\\nCalmen that small of commandly and feers, hull am word,\\nThat thicked manage into thyself?\\n\\nWARWICK:\\nNo tongue to my going to another way.\\n\\nDUKE:\\nThe which your hand, your grace compell'd his subjects:\\nBian no more fellow\"\n",
      " b\"ROMEO:\\nBy Bight nor hencefort, hearken here!\\nTut, dear God, for this unach o' the capars\\nOf Exeter?\\n\\nCORIOLANUS:\\nBut he's gone.\\n\\nClown:\\nO let it not confess set it down.\\n\\nSICINIUS:\\nSpeakest thou, my lord.\\n\\nKING RICHARD II:\\nSo proudly amping.\\nPedrance hast thou aspire my hell.\\nCome, my son, in the bosom of thoss of ours.\\n\\nKING EDWARD IV:\\nYea, brother, a surper, sir, can do good\\nFrom whence, he hath no willing to that frin,\\nForget' to me.'\\n\\nPETRUCHIO:\\nYou love me, Lord dear dignity, a\\ngentlewoman of customan. The blood\\nIs fally upon this palace hours Marsh well\\nShe wills in presence on What with\\nhe thinks my warlike brows but sudden;\\nBy confiscats; some hour full times to gied;\\nFor one condemn how thy beauty shall died\\nFor mercy for unskeath: he, sir, his commands\\nAs great as quickly Paris. They fear\\nThe lords o' your thief orders to weep up: let's hence\\nThe needless weal overtain'd tongue\\nFrom old Bohemia bends the king's,\\nAnd she knew with the king and Qures:\\nBut is a more than when it was m\"\n",
      " b\"ROMEO:\\nGreat Abolo, that I am now.\\n\\nShepherd:\\nReself, as if Kethonks, as they to purgeance\\nFor nive to speak in me: but suddenly he wear a\\ncruel tent; prevent thee!\\nDercy Solimine!\\nWill I le, his hair of fear; and\\nwe hear no store on with death,\\nbut alterity on every night.\\nNow, Warwick, then as concease?\\n\\nBIONDELLO:\\nHe does be made more penitent I feel your majesty\\nAs here, by neighbour and rises in eat,\\nThat both returness to them.\\n\\nJULIET:\\nSo plunce your pleasure wise?\\nWith Pilate-fevert dances! married there?\\n\\nSecond Murderer:\\nHusband, let's follow:\\nShe where he stands upon your daughter?\\n\\nPETRUCHIO:\\nI pray, sir, sir, I know all ours.\\n\\nBRUTUS:\\nIt will give you the first honest.\\n\\nERCALUS:\\nNo, I am thus?'\\nMost bone tyrants! freely out\\nThe place: any things, the old sexentes\\nhadming briden: he kills of hell.\\n\\nHENRY BOLINGBROKE:\\nSay that I have to grant for your souls.\\n\\nPETRUCHIO:\\nThere is a winged from this ear\\nFrom young and empty taste Henry, for myself sir.\\n\\nCOMINIUS:\\nI thank the gods! O\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.3603463172912598\n"
     ]
    }
   ],
   "source": [
    "# batch text generation\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bde3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f186c09e8b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:56:19.872573: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "# save/load model\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf6fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "O that came four thousand city!\n",
      "\n",
      "CAMILLO:\n",
      "Besides the people whipp\n",
      "In thoughts of math obedient wit\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34645c5b",
   "metadata": {},
   "source": [
    "# customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebaf086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a53babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c903f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "307119b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 6s 25ms/step - loss: 2.7172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f186c090f70>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b14d325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1580\n",
      "Epoch 1 Batch 50 Loss 2.0946\n",
      "Epoch 1 Batch 100 Loss 1.9529\n",
      "Epoch 1 Batch 150 Loss 1.8900\n",
      "\n",
      "Epoch 1 Loss: 1.9991\n",
      "Time taken for 1 epoch 5.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8448\n",
      "Epoch 2 Batch 50 Loss 1.7568\n",
      "Epoch 2 Batch 100 Loss 1.6602\n",
      "Epoch 2 Batch 150 Loss 1.6588\n",
      "\n",
      "Epoch 2 Loss: 1.7273\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.6104\n",
      "Epoch 3 Batch 50 Loss 1.5547\n",
      "Epoch 3 Batch 100 Loss 1.5241\n",
      "Epoch 3 Batch 150 Loss 1.5220\n",
      "\n",
      "Epoch 3 Loss: 1.5648\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.5014\n",
      "Epoch 4 Batch 50 Loss 1.5066\n",
      "Epoch 4 Batch 100 Loss 1.4345\n",
      "Epoch 4 Batch 150 Loss 1.4614\n",
      "\n",
      "Epoch 4 Loss: 1.4629\n",
      "Time taken for 1 epoch 4.92 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3958\n",
      "Epoch 5 Batch 50 Loss 1.4166\n",
      "Epoch 5 Batch 100 Loss 1.3749\n",
      "Epoch 5 Batch 150 Loss 1.3548\n",
      "\n",
      "Epoch 5 Loss: 1.3927\n",
      "Time taken for 1 epoch 5.17 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3563\n",
      "Epoch 6 Batch 50 Loss 1.3160\n",
      "Epoch 6 Batch 100 Loss 1.3259\n",
      "Epoch 6 Batch 150 Loss 1.3346\n",
      "\n",
      "Epoch 6 Loss: 1.3393\n",
      "Time taken for 1 epoch 4.93 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2963\n",
      "Epoch 7 Batch 50 Loss 1.3290\n",
      "Epoch 7 Batch 100 Loss 1.2842\n",
      "Epoch 7 Batch 150 Loss 1.2879\n",
      "\n",
      "Epoch 7 Loss: 1.2950\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2539\n",
      "Epoch 8 Batch 50 Loss 1.2325\n",
      "Epoch 8 Batch 100 Loss 1.2976\n",
      "Epoch 8 Batch 150 Loss 1.3199\n",
      "\n",
      "Epoch 8 Loss: 1.2547\n",
      "Time taken for 1 epoch 5.03 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1921\n",
      "Epoch 9 Batch 50 Loss 1.2170\n",
      "Epoch 9 Batch 100 Loss 1.2636\n",
      "Epoch 9 Batch 150 Loss 1.2616\n",
      "\n",
      "Epoch 9 Loss: 1.2155\n",
      "Time taken for 1 epoch 5.22 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1355\n",
      "Epoch 10 Batch 50 Loss 1.1758\n",
      "Epoch 10 Batch 100 Loss 1.1485\n",
      "Epoch 10 Batch 150 Loss 1.1761\n",
      "\n",
      "Epoch 10 Loss: 1.1768\n",
      "Time taken for 1 epoch 5.33 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539497f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
