{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b30384",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0a2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:10:11.090737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 18:10:11.566134: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/lib64::/home/rmaia/miniconda3/envs/dim0494/lib/:/home/rmaia/miniconda3/envs/dim0494/lib/\n",
      "2023-05-16 18:10:11.566199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/lib64::/home/rmaia/miniconda3/envs/dim0494/lib/:/home/rmaia/miniconda3/envs/dim0494/lib/\n",
      "2023-05-16 18:10:11.566205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow and other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde89fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the Sheakespeare dataset\n",
    "path_to_file ='shakespeare.txt'\n",
    "if os.path.exists(path_to_file) is False:\n",
    "    path_to_file =tf.keras.utils.get_file(\n",
    "        'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bcbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af63cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc9c0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'The data has {len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220523c",
   "metadata": {},
   "source": [
    "# text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e597c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, you need to convert the strings to a numerical representation.\n",
    "\n",
    "# The tf.keras.layers.StringLookup layer can convert each character into a numeric ID.\n",
    "# It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17db3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text examples: ['Life is not fair.', 'But you have to deal with it.']\n",
      "\n",
      "Tokenized version: <tf.RaggedTensor [[b'L', b'i', b'f', b'e', b' ', b'i', b's', b' ', b'n', b'o', b't', b' ',\n",
      "  b'f', b'a', b'i', b'r', b'.']                                          ,\n",
      " [b'B', b'u', b't', b' ', b'y', b'o', b'u', b' ', b'h', b'a', b'v', b'e',\n",
      "  b' ', b't', b'o', b' ', b'd', b'e', b'a', b'l', b' ', b'w', b'i', b't',\n",
      "  b'h', b' ', b'i', b't', b'.']                                          ]>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of text\n",
    "example_texts = ['Life is not fair.', 'But you have to deal with it.']\n",
    "\n",
    "# tokenization\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "\n",
    "# print results\n",
    "print(f'Text examples: {example_texts}\\n')\n",
    "print(f'Tokenized version: {chars}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9bf8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the tf.keras.layers.StringLookup layer:\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab),\n",
    "    mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93875ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the ids of the examples above: <tf.RaggedTensor [[25, 48, 45, 44, 2, 48, 58, 2, 53, 54, 59, 2, 45, 40, 48, 57, 9],\n",
      " [15, 60, 59, 2, 64, 54, 60, 2, 47, 40, 61, 44, 2, 59, 54, 2, 43, 44, 40,\n",
      "  51, 2, 62, 48, 59, 47, 2, 48, 59, 9]                                   ]>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the layer above to convert characters into numbers\n",
    "ids = ids_from_chars(chars)\n",
    "print(f'These are the ids of the examples above: {ids}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3eb44",
   "metadata": {},
   "source": [
    "Since the goal is to generate text, it will also be important to invert this representation and recover human-readable strings from it.\n",
    "\n",
    "For this you can use tf.keras.layers.StringLookup(..., invert=True).\n",
    "\n",
    "Note: Here instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cfcb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the inversion layer\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de844b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the chars converted from ids: <tf.RaggedTensor [[b'L', b'i', b'f', b'e', b' ', b'i', b's', b' ', b'n', b'o', b't', b' ',\n",
      "  b'f', b'a', b'i', b'r', b'.']                                          ,\n",
      " [b'B', b'u', b't', b' ', b'y', b'o', b'u', b' ', b'h', b'a', b'v', b'e',\n",
      "  b' ', b't', b'o', b' ', b'd', b'e', b'a', b'l', b' ', b'w', b'i', b't',\n",
      "  b'h', b' ', b'i', b't', b'.']                                          ]>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the ids back into characters\n",
    "chars = chars_from_ids(ids)\n",
    "print(f'These are the chars converted from ids: {chars}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebc65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the examples (text) recoved from ids: [b'Life is not fair.' b'But you have to deal with it.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can tf.strings.reduce_join to join the characters back into strings.\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "# let's test it\n",
    "t1 = text_from_ids(ids)\n",
    "print(f'These are the examples (text) recoved from ids: {t1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0383f",
   "metadata": {},
   "source": [
    "# Create training examples and targets\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f55e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the indices from the entire text : [19 48 57 ... 46  9  1]\n",
      "\n",
      "Number of indices : 1115394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text -> stream of indices from the entire text.\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(f'These are the indices from the entire text : {all_ids}\\n')\n",
    "print(f'Number of indices : {len(all_ids)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f71ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset of indices\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b02cf50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 19, corresponding character: F\n",
      "id: 48, corresponding character: i\n",
      "id: 57, corresponding character: r\n",
      "id: 58, corresponding character: s\n",
      "id: 59, corresponding character: t\n",
      "id: 2, corresponding character:  \n",
      "id: 16, corresponding character: C\n",
      "id: 48, corresponding character: i\n",
      "id: 59, corresponding character: t\n",
      "id: 48, corresponding character: i\n",
      "id: 65, corresponding character: z\n",
      "id: 44, corresponding character: e\n",
      "id: 53, corresponding character: n\n",
      "id: 11, corresponding character: :\n",
      "id: 1, corresponding character: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check on the what the dataset gives us\n",
    "for ids in ids_dataset.take(15):\n",
    "    print(f'id: {ids}, corresponding character: {chars_from_ids(ids).numpy().decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d610c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the textual sequences to be used\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51ca34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# The batch method lets you easily convert these individual characters to sequences of the desired size.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "379d5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# It's easier to see what this is doing if you join the tokens back into strings:\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc7f92",
   "metadata": {},
   "source": [
    "For training you'll need a dataset of (input, label) pairs.\n",
    "Where input and label are sequences.\n",
    "\n",
    "At each time step the input is the current character and the label is the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7339140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If my input is \"Tensorflow\":\n",
      " Input: ['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o']\n",
      "Output: ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w']\n"
     ]
    }
   ],
   "source": [
    "# Here's a function that takes a sequence as input, duplicates,\n",
    "# and shifts it to align the input and label for each timestep:\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# let's test it\n",
    "o1 = split_input_target(list(\"Tensorflow\"))\n",
    "print(f'If my input is \\\"Tensorflow\\\":\\n Input: {o1[0]}\\nOutput: {o1[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b674d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply that function to the dataset we created\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "281f7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :\n",
      " b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target:\n",
      " b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Input :\n",
      " b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target:\n",
      " b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# letś test i\n",
    "for input_example, target_example in dataset.take(2):\n",
    "    print(f'Input :\\n {text_from_ids(input_example).numpy()}')\n",
    "    print(f'Target:\\n {text_from_ids(target_example).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3b53e",
   "metadata": {},
   "source": [
    "# Create training batches\n",
    "\n",
    "You used tf.data to split the text into manageable sequences.\n",
    "\n",
    "But before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e38cfa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fd1b1",
   "metadata": {},
   "source": [
    "# Build The Model\n",
    "\n",
    "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).\n",
    "\n",
    "This model has three layers:\n",
    "\n",
    "tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "\n",
    "tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "\n",
    "tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3543afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "649f87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 rnn_units):\n",
    "        super().__init__(self)\n",
    "    \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self,\n",
    "             inputs,\n",
    "             states=None,\n",
    "             return_state=False,\n",
    "             training=False):\n",
    "        \n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "193e2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed0bf6",
   "metadata": {},
   "source": [
    "# Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dfe32b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 19:01:22.203311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# First check the shape of the output:\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a74c531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408962d",
   "metadata": {},
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "159a837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are indices taken from the output of the model: [13 64 40 58 20 17 17 19 21 16 11 46 57 13 56  0 50 65 32 13 30  2 63 33\n",
      " 51  1 16 29 48 40 14 60 46  2 21  8 23 19  6 65 52 12 51 38 31 26 29 65\n",
      " 29  2 65 12  8 15 60  6 57 49 10 30 57 34  8 62 57 28 41 32 20 42 19 40\n",
      " 65  8  8 35 42  3 30 20 55  6 40 58 38 15 62 41  4 33 40 63 30 63  3 38\n",
      " 34 64 43 63]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's draw one sample\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "\n",
    "# transform that into a numpy sequence\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "# print it\n",
    "print(f'These are indices taken from the output of the model: {sampled_indices}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2e3da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      ", b'our, I know not well what they\\nare: but precise villains they are, that I am sure\\nof; and void of al'\n",
      "\n",
      "Next Char Predictions:\n",
      ", b\"?yasGDDFHC:gr?q[UNK]kzS?Q xTl\\nCPiaAug H-JF'zm;lYRMPzP z;-Bu'rj3QrU-wrObSGcFaz--Vc!QGp'asYBwb$TaxQx!YUydx\"\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted by this untrained model:\n",
    "print(f'Input:\\n, {text_from_ids(input_example_batch[0]).numpy()}\\n')\n",
    "print(f'Next Char Predictions:\\n, {text_from_ids(sampled_indices).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7adbcb",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "At this point the problem can be treated as a standard classification problem.\n",
    "Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
    "\n",
    "Attach an optimizer, and a loss function\n",
    "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because your model returns logits, you need to set the from_logits flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f66a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e912f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 100, 66)\n",
      "Mean loss:        4.188908576965332\n"
     ]
    }
   ],
   "source": [
    "# test the cost function on a batch of samples\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(f'Prediction shape: {example_batch_predictions.shape}') # (batch_size, sequence_length, vocab_size)\")\n",
    "print(f'Mean loss:        {example_batch_mean_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93df728",
   "metadata": {},
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes.\n",
    "\n",
    "To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size.\n",
    "\n",
    "A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e59084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.95077"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba48600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295ea01",
   "metadata": {},
   "source": [
    "# Configure checkpoints\n",
    "\n",
    "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1272ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d67753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e9e1fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 22:12:14.241016: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-05-16 22:12:14.241072: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_5' defined at (most recent call last):\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8264/4024136925.py\", line 2, in <module>\n      history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_5'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_5}}]] [Op:__inference_train_function_3092]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dim0494/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_5' defined at (most recent call last):\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8264/4024136925.py\", line 2, in <module>\n      history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/rmaia/miniconda3/envs/dim0494/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_5'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_5}}]] [Op:__inference_train_function_3092]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04baa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation model\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09448bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d49e5103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Villain, stir! strike! do you know this countryman:\n",
      "The swift are and of thyself none at the clusch,\n",
      "What treachery face thrusts no harm such sworn,\n",
      "Which, though thou wilt obedien: this time that\n",
      "As after reward, or that he knews,\n",
      "You worldle steel, thus\n",
      "Glood'd unto justice footing to the tenter as\n",
      "we hear himself.' but, then, I do not\n",
      "Too hot to pardon Rome! humbly thou\n",
      "affript the crown.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "What would your genitate here?\n",
      "\n",
      "O God,\n",
      "What news, invoint, my tent?\n",
      "\n",
      "CATESBY:\n",
      "Now, Words, report is cold of his the covertood; your\n",
      "hongiers. You are a Roman;\n",
      "For sleep my slave: where he should hear him stand:\n",
      "This is a great deputy, but the wish of breath,\n",
      "That he hath done met, though a cup of Rosaline.\n",
      "\n",
      "RIVERS:\n",
      "And therefore be rilier, another cause.\n",
      "\n",
      "GLOUCESTER:\n",
      "What think so fars? Is the beauty o'er-heads\n",
      "our foe, and beat your parts, York and Edward's love,\n",
      "Immedies 'gainst the throne a heavenly hate;\n",
      "And show no warrant changing ill:\n",
      "Is it thou corclivites me in his stake  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.932978630065918\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c57ee649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe which my sues, you are like gozer Marcius,\\nEre we were all unhappier than you, against his strong:\\nIre parted to reply, at the lie I'll lady\\nAnd still to purpose and virtuous wrongs\\nAnd downright by thine. And as I turn:\\nTo hon, I, believe not this beful mine.\\n\\nRICHMOND:\\nGod and hang'd children not the chirace of ourselves:\\nWhy, then, as I do priso leave myself to\\ndance.\\n\\nPAGE:\\nThis proud infect what dangers or others, parts,\\nYet that this mould be to stand, and be seen to be;\\nFor me, that he be lost in heaven;\\nThe comfort, do not say he is.\\n\\nGLOUCESTER:\\nWhat, would you have meal due? when you have must one\\nand honour to my clothes till now\\nIn Margaret's sacred household room exile\\nThat down while heaven confine you,\\nShall satisfied that deadly was but a piteous crown,\\nAnd never toward the nornes. The neal of tears!\\n\\nWARWICK:\\nThen this, good friend! when every story reado\\nI have spent my knot in blood and pity\\nSomewhing your bones that Ruts and rotteness\\nTo this conventent that th\"\n",
      " b\"ROMEO:\\nFor being one another, boy.\\n\\nClown:\\nHold, hold!\\nI can be a hand is thence?\\n\\nMENENIUS:\\nI would you have made no friend to deep upon thee.\\nThe statues of a complessiob like me.\\n\\nPETRUCHIO:\\nI prithee, my lord. What says my master!\\n\\nDERBY:\\nMy noble Plantagenet,--\\n\\nDUCHESS OF YORK:\\n\\nHENRY BOLINGBROKE:\\nGo have your hands;\\nFor I have muse accuse thee with the air.\\n\\nBIONDELLO:\\nHe does got in Peace, sir, he has done now,\\nNot up a dream of heaven? the case as you,\\nAs I have eyes conceived with sorrow's middless death\\nAnd every meaner sense outward wothing clouds,\\nAs if this hurly I in great apollo.\\n\\nROMEO:\\nHe doth it in the pempless of the air!\\n\\nJULIET:\\nNo, no: but having, march on join of justice brain warriors,\\nWith given such corse, as woman's faces cheers forgot,\\nBut, for, in towards our Rome hath resisted my rich,\\nThere any of it: it may be absent,\\nYou shakest in some sick heavens to this If.\\nIn my trumper's eyes ungovy 'grazen;\\nThis is the executioner.\\n\\nGLOUCESTER:\\nWe thank your grace. I'\"\n",
      " b\"ROMEO:\\nVillain, I see, which needs me weep,\\nSo they at Pearable gentleman:\\nDraws thee for my maner? what a torment I do?\\n\\nDUKE VINCENTIO:\\nIf it peer me along.\\n\\nProvost:\\nI do not know what strict hardly given to her:\\nThings traitor, Kate, and are there,\\nMay be against in vain.\\nThou talk'd, a husband to command;\\nPoor stucing surse, but with usurper revenge\\nBut newly in the high of war,\\nAs yonder looks and honour from hen,\\nDo me return to live.\\n\\nGREMIO:\\nBend thy cold and daughter, your name, the son,\\nYour full of death, my books and let your mejesh\\nTo hear their cruel strength and fine shall visit\\nNothing that death he marks that placed his charge;\\nAnd she came untimely from to death,\\n'Tas I possess'd, in Warwick.\\n\\nTRANIO:\\nThe most struck him.\\n\\nSICINIUS:\\nHelp not pale.\\n\\nGRUMIO:\\nCalmen that small of commandly and feers, hull am word,\\nThat thicked manage into thyself?\\n\\nWARWICK:\\nNo tongue to my going to another way.\\n\\nDUKE:\\nThe which your hand, your grace compell'd his subjects:\\nBian no more fellow\"\n",
      " b\"ROMEO:\\nBy Bight nor hencefort, hearken here!\\nTut, dear God, for this unach o' the capars\\nOf Exeter?\\n\\nCORIOLANUS:\\nBut he's gone.\\n\\nClown:\\nO let it not confess set it down.\\n\\nSICINIUS:\\nSpeakest thou, my lord.\\n\\nKING RICHARD II:\\nSo proudly amping.\\nPedrance hast thou aspire my hell.\\nCome, my son, in the bosom of thoss of ours.\\n\\nKING EDWARD IV:\\nYea, brother, a surper, sir, can do good\\nFrom whence, he hath no willing to that frin,\\nForget' to me.'\\n\\nPETRUCHIO:\\nYou love me, Lord dear dignity, a\\ngentlewoman of customan. The blood\\nIs fally upon this palace hours Marsh well\\nShe wills in presence on What with\\nhe thinks my warlike brows but sudden;\\nBy confiscats; some hour full times to gied;\\nFor one condemn how thy beauty shall died\\nFor mercy for unskeath: he, sir, his commands\\nAs great as quickly Paris. They fear\\nThe lords o' your thief orders to weep up: let's hence\\nThe needless weal overtain'd tongue\\nFrom old Bohemia bends the king's,\\nAnd she knew with the king and Qures:\\nBut is a more than when it was m\"\n",
      " b\"ROMEO:\\nGreat Abolo, that I am now.\\n\\nShepherd:\\nReself, as if Kethonks, as they to purgeance\\nFor nive to speak in me: but suddenly he wear a\\ncruel tent; prevent thee!\\nDercy Solimine!\\nWill I le, his hair of fear; and\\nwe hear no store on with death,\\nbut alterity on every night.\\nNow, Warwick, then as concease?\\n\\nBIONDELLO:\\nHe does be made more penitent I feel your majesty\\nAs here, by neighbour and rises in eat,\\nThat both returness to them.\\n\\nJULIET:\\nSo plunce your pleasure wise?\\nWith Pilate-fevert dances! married there?\\n\\nSecond Murderer:\\nHusband, let's follow:\\nShe where he stands upon your daughter?\\n\\nPETRUCHIO:\\nI pray, sir, sir, I know all ours.\\n\\nBRUTUS:\\nIt will give you the first honest.\\n\\nERCALUS:\\nNo, I am thus?'\\nMost bone tyrants! freely out\\nThe place: any things, the old sexentes\\nhadming briden: he kills of hell.\\n\\nHENRY BOLINGBROKE:\\nSay that I have to grant for your souls.\\n\\nPETRUCHIO:\\nThere is a winged from this ear\\nFrom young and empty taste Henry, for myself sir.\\n\\nCOMINIUS:\\nI thank the gods! O\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.3603463172912598\n"
     ]
    }
   ],
   "source": [
    "# batch text generation\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bde3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f186c09e8b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:56:19.872573: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "# save/load model\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf6fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "O that came four thousand city!\n",
      "\n",
      "CAMILLO:\n",
      "Besides the people whipp\n",
      "In thoughts of math obedient wit\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34645c5b",
   "metadata": {},
   "source": [
    "# customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebaf086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a53babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c903f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "307119b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 6s 25ms/step - loss: 2.7172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f186c090f70>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b14d325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1580\n",
      "Epoch 1 Batch 50 Loss 2.0946\n",
      "Epoch 1 Batch 100 Loss 1.9529\n",
      "Epoch 1 Batch 150 Loss 1.8900\n",
      "\n",
      "Epoch 1 Loss: 1.9991\n",
      "Time taken for 1 epoch 5.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8448\n",
      "Epoch 2 Batch 50 Loss 1.7568\n",
      "Epoch 2 Batch 100 Loss 1.6602\n",
      "Epoch 2 Batch 150 Loss 1.6588\n",
      "\n",
      "Epoch 2 Loss: 1.7273\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.6104\n",
      "Epoch 3 Batch 50 Loss 1.5547\n",
      "Epoch 3 Batch 100 Loss 1.5241\n",
      "Epoch 3 Batch 150 Loss 1.5220\n",
      "\n",
      "Epoch 3 Loss: 1.5648\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.5014\n",
      "Epoch 4 Batch 50 Loss 1.5066\n",
      "Epoch 4 Batch 100 Loss 1.4345\n",
      "Epoch 4 Batch 150 Loss 1.4614\n",
      "\n",
      "Epoch 4 Loss: 1.4629\n",
      "Time taken for 1 epoch 4.92 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3958\n",
      "Epoch 5 Batch 50 Loss 1.4166\n",
      "Epoch 5 Batch 100 Loss 1.3749\n",
      "Epoch 5 Batch 150 Loss 1.3548\n",
      "\n",
      "Epoch 5 Loss: 1.3927\n",
      "Time taken for 1 epoch 5.17 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3563\n",
      "Epoch 6 Batch 50 Loss 1.3160\n",
      "Epoch 6 Batch 100 Loss 1.3259\n",
      "Epoch 6 Batch 150 Loss 1.3346\n",
      "\n",
      "Epoch 6 Loss: 1.3393\n",
      "Time taken for 1 epoch 4.93 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2963\n",
      "Epoch 7 Batch 50 Loss 1.3290\n",
      "Epoch 7 Batch 100 Loss 1.2842\n",
      "Epoch 7 Batch 150 Loss 1.2879\n",
      "\n",
      "Epoch 7 Loss: 1.2950\n",
      "Time taken for 1 epoch 4.96 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2539\n",
      "Epoch 8 Batch 50 Loss 1.2325\n",
      "Epoch 8 Batch 100 Loss 1.2976\n",
      "Epoch 8 Batch 150 Loss 1.3199\n",
      "\n",
      "Epoch 8 Loss: 1.2547\n",
      "Time taken for 1 epoch 5.03 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1921\n",
      "Epoch 9 Batch 50 Loss 1.2170\n",
      "Epoch 9 Batch 100 Loss 1.2636\n",
      "Epoch 9 Batch 150 Loss 1.2616\n",
      "\n",
      "Epoch 9 Loss: 1.2155\n",
      "Time taken for 1 epoch 5.22 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1355\n",
      "Epoch 10 Batch 50 Loss 1.1758\n",
      "Epoch 10 Batch 100 Loss 1.1485\n",
      "Epoch 10 Batch 150 Loss 1.1761\n",
      "\n",
      "Epoch 10 Loss: 1.1768\n",
      "Time taken for 1 epoch 5.33 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539497f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
