{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0a2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 11:28:54.400884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 11:28:54.527120: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-03 11:28:55.038554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/rmaia/miniconda3/envs/dim0494/lib/\n",
      "2022-11-03 11:28:55.038617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/rmaia/miniconda3/envs/dim0494/lib/\n",
      "2022-11-03 11:28:55.038624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import timeimport tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde89fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file ='shakespeare.txt'\n",
    "if os.exist(oath_to_file) is False:\n",
    "    path_to_file =tf.keras.utils.get_file(\n",
    "        'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bcbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af63cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9c0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17db3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 11:29:35.563955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:35.600018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:35.600327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:35.601414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 11:29:35.601726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:35.602031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:35.602331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:36.027737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:36.027933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:36.028076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:29:36.028195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3256 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:03:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the strings to a numerical representation.\n",
    "# The tf.keras.layers.StringLookup layer can convert each character into a numeric ID.\n",
    "# It just needs the text to be split into tokens first.\n",
    "# example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bf8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters -> numbers (layer)\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93875ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It converts from tokens to character IDs:\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfcb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers -> ids (layer)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de844b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numbers -> characters\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebc65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join characters back into text (layer)\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7369c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numbers into text\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f55e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text -> stream of indices (numbers)\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f71ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02cf50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d610c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the textual sequences to be used\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d51ca34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379d5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7339140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training you'll need a dataset of (input, label) pairs.\n",
    "# Where input and label are sequences.\n",
    "# At each time step the input is the current character and the label is the next character.\n",
    "# Here's a function that takes a sequence as input, duplicates, and shifts it\n",
    "# to align the input and label for each timestep:\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e75c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b674d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "281f7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38cfa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3543afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649f87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "193e2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dfe32b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 11:30:33.279841: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# trying the model\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a74c531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "159a837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get actual predictions from the model you need to sample from the output distribution,\n",
    "#to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940e9460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30, 52, 43, 40, 52, 50, 55,  9, 46, 37, 41, 29, 62, 58, 51, 63,\n",
       "       39, 38, 46, 10,  2, 49, 28, 41, 29,  8, 48, 37, 52, 57, 48, 32, 35,\n",
       "       45, 35, 15, 33, 44, 56, 29, 51, 57, 22, 15, 51, 52, 25, 42, 43, 34,\n",
       "       16, 15, 61,  7, 15, 24, 39,  2,  2, 30, 43, 58, 25,  6,  5, 47, 64,\n",
       "       65, 28, 61, 63, 37,  1, 54, 38, 35, 21, 11, 20, 24, 29,  4, 46, 24,\n",
       "        6, 35, 64, 50, 24, 55, 20,  3, 36, 26,  7, 19,  2,  4, 62])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2e3da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'certain, but this certain,\\nThat, if thou conquer Rome, the benefit\\nWhich thou shalt thereby reap is '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"QQmdamkp.gXbPwslxZYg3 jObP-iXmriSVfVBTeqPlrIBlmLcdUCBv,BKZ  QdsL'&hyzOvxX\\noYVH:GKP$gK'VykKpG!WM,F $w\"\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted by this untrained model:\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f66a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e912f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.189691, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# cost (untrained)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e59084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.002396"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba48600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1272ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d67753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e9e1fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 12s 64ms/step - loss: 1.3685\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 12s 64ms/step - loss: 1.3172\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 1.2715\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.2301\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 12s 64ms/step - loss: 1.1907\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.1502\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.1081\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 1.0642\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 1.0178\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.9686\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.9185\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.8675\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.8162\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.7667\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.7185\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.6753\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6378\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6039\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5742\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 0.5493\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04baa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation model\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09448bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d49e5103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "O monstrupt, look, he; 'tis true: I have deserved no other\n",
      "By her horse to every dangerous,\n",
      "I do repent their works to begif.\n",
      "When came down and full of justice:\n",
      "The testy greedifes were end;\n",
      "There the interpetue or your crown, could not have your\n",
      "called bears: it was murdered there; which, I'll make you\n",
      "The Earl of Somerset? and ha!\n",
      "The duty that doth falch of last ninet'-foner?\n",
      "\n",
      "Lord:\n",
      "We have you farther.\n",
      "\n",
      "PAULINA:\n",
      "I did.\n",
      "\n",
      "Provost:\n",
      "A callais\n",
      "The wars and harbonr'd from the Roman camp contrary,\n",
      "Where sister many friends the worthy fellow: mean\n",
      "to cry, as in haste at hours.\n",
      "\n",
      "CORIOLANUS:\n",
      "You be sure of these and\n",
      "three-and that which you have possessed you with his good will\n",
      "Or that I stould do not sell him.\n",
      "\n",
      "Shepherd:\n",
      "I would these words he has possess'd it?\n",
      "\n",
      "SICINIUS:\n",
      "Now tell me, if you please;\n",
      "You have possessed with false grief.\n",
      "O, peace in fortul high'd fortune,\n",
      "Suffices i' the right difty love,\n",
      "That I have possess'd from his oath\n",
      "To banish young Marcius' forced bags:\n",
      "The bish of  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.8576488494873047\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c57ee649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nFor Warwick carried Rome and true succession,\\nWhich you do love thee, and her like upon him.\\n\\nGREMIO:\\nYes, sweet Warwick, turn you talk.\\n\\nQUEEN MARGARET:\\nNo prince, impude him! I have deserved not\\nThat your kindred shuns your sister should come to me,\\nAs you were found it,--I had it, I will vengures\\nSinnible than my guest, beggar to the\\nspeaks: no, not by my presence, servants, will unpresset her.\\n\\nQUEEN MARGARET:\\nWhat, Juliet!\\n\\nCLAUDIO:\\nThey say, we will welcome impossible patience:\\nIf she can live in fore a heil, and must die\\nfires it will make in your own despite of me\\nTo stay here in the good o' the opening.\\nIs well seen them in the aution of my wrath:\\nBy this disgracious strumping to him, he's all\\nReceives the field-fast-grave we learn\\nThe murder'd moolship in them are the thirt.\\n\\nDUKE OF AUMERLE:\\nI do beseech you, sir, as if I had given his commanded so\\nOpen thus; your affection with your side,\\nWith wealth eyes speeches of my fortune;\\nI am an o'erjore foolery. For Whose deptshir\"\n",
      " b\"ROMEO:\\nConsul, that all this chat escape!\\n\\nAEdile:\\nList, to sword, I am sure,\\nTo be set on his chambs and green spirits\\non't.\\n\\nMARCIUS:\\nThe gods forbid!\\nI prithee, go in! him your counsel to?\\n\\nBIANCA:\\nCambio; no harm the mark of lead.\\nThink'st thou out my service, which elder death\\nHad hard by his subjects to their subjects' magks;\\nThat is, by garlic: spare howly continuence!\\nThis for my bonds that halthess born to him,\\nBecqualitied a marshea that goes water, birth, sension,\\nMy best rance, and I know thy hand,\\nThis thrive of sacred issue from the war\\nDesperitent in their deaths that were yourselves\\nTo give them the intended fruit.\\nYour brother's short, we at the crown,\\nYou have been much, I saw the people Hew-\\nShe shall not nothing; you are requite to sleepy.\\n\\nGLOUCESTER:\\nUp to issue for myself, for a nature begins:\\n'Tis not the good a restrain.\\nBut what of that! Good Lucentio,\\nHow do you now, you do not purple to arm,\\nWhile it his son's ransomabey.\\n\\nShepherd:\\nI cannot stay by him a creature\"\n",
      " b\"ROMEO:\\nThe narrow-like and objocural down,\\nHath almost season with me. Boy! frightin, a wife: your choice o' the last\\nIncuparable, and I will comporaught me,\\nHe's summer than the ire through whether than it?\\n\\nCATESBY:\\nMy lord!\\n\\nKING RICHARD III:\\nThe ligut of Frunce, a rapier's poison,\\nHeaven serve, would none of you than I will accept.\\n\\nLUCENTIO:\\nArms, almost to you! The wind\\nshould wish! You are the terror of your soul!\\nThy friends splander, stopy his companion\\nThe worth of hooping cheeks o' the city\\nWith the anger of his timeless feats?\\n\\nWARWICK:\\nDecress then we have proclaim'd so prison,\\nAlthough I shall tell the knave by him.\\n\\nJULIET:\\nWhat is the mother of this cause?\\nWhy, you have always have behnow the heavy digges mouth:\\nThey'll tardy signal of thrifer\\nTo one before a man than others. Ang long again,\\nAline and down one.\\n\\nPirst Officer:\\nThe time to come, we see the crown if ady more love\\nThan Bolingbroke is changed: not a party, to brig\\nhim profited to in me so many old man's\\nhands yet\"\n",
      " b\"ROMEO:\\nMy wife is more than you'ld thy desire\\nThe market and abuse and uncher?\\n\\nTRANIO:\\nI fear it is the marriage of my pay and drif:\\nNot I: but for a noble case,\\nThe second cock hurt father's lander.\\nThose other was but hearing or would find\\nCommiting amity made for war\\nLucentim the queen burling them to oract.\\n\\nBRUTUS:\\nWhips your honour will.\\n\\nFLORIZEL:\\nI say unto you,\\nFor making me with slivio; some slain in war,\\nFor in his beard to Proshumbled, should be side\\nThan near thus coldlence; and therewithal\\nCame from Lord Angelo do apparent\\nTo shall not still have rack'd that one of your worship's house.\\nI am against my holy privery bold, both haste:\\nHe being greater, I challen yound friend?\\n\\nHASTINGS:\\nMy lord!\\n\\nBENVOLIO:\\nIn ten times so, but not my servant.\\n\\nProvost:\\nAnd I, that she is now; I say tongues so.\\n\\nCORIOLANUS:\\nI mean, sir, I know no further.\\n\\nCORIOLANUS:\\nI did\\nMay suffer you to seek him thought or else\\nTo hopeler tribute of your gracious daughter.\\n\\nGRUMIO:\\nAnd so did I.\\n\\nROMEO:\\nWhit\"\n",
      " b\"ROMEO:\\nO mine, I shall not entreat the winds of pity.\\n\\nPRINCE EDWARD:\\nI know she is a measure to the inform to\\nhim even in the covert of the priest.\\n\\nWARWICK:\\nSuch news, my hearts! come, madam,\\nI dare my life must all the world I should hear me.\\nYou know my servants and your will, good night!\\nThis bud life, like a Stanking speaks!\\n\\nWARWICK:\\nWhat pleasant that we flatter'd me a thing your place,\\nTo be so bold to hunt, belike, you know the world\\nMy either feel'd my father's approbation.\\n\\nCORIOLANUS:\\nFor the best of the king I king!\\n\\nHENRY BOLINGBROKE:\\nBring forth the sea but by the yield;\\nSince think'st thou without means to kill my holy grave;\\nHad theirs and yours, set down the corpose:\\nThy obedies all ease, and three old man blood,\\nTo fight in safeguard of your judgment, stay\\nThere the wild waters; and the dead names and case\\nMy pings are full of day.\\n\\nDUKE OF AUMERLE:\\nGood Aufidius that deprees praise upon themselves,\\nWith pure like substsmance in the veins of Junis,\\nyour father's small sti\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.738903522491455\n"
     ]
    }
   ],
   "source": [
    "# batch text generation\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bde3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f08500d71c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "# save/load model\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baf6fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Who am pressing her with kindred slavess of steel,\n",
      "Romeoping him hath sendon very slaves\n",
      "Who play'd\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34645c5b",
   "metadata": {},
   "source": [
    "# customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebaf086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a53babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c903f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "307119b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 13s 62ms/step - loss: 2.6928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07ec1f6850>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b14d325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1682\n",
      "Epoch 1 Batch 50 Loss 2.0298\n",
      "Epoch 1 Batch 100 Loss 1.9279\n",
      "Epoch 1 Batch 150 Loss 1.8772\n",
      "\n",
      "Epoch 1 Loss: 1.9732\n",
      "Time taken for 1 epoch 12.17 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8107\n",
      "Epoch 2 Batch 50 Loss 1.7492\n",
      "Epoch 2 Batch 100 Loss 1.6422\n",
      "Epoch 2 Batch 150 Loss 1.6227\n",
      "\n",
      "Epoch 2 Loss: 1.6965\n",
      "Time taken for 1 epoch 11.70 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.5887\n",
      "Epoch 3 Batch 50 Loss 1.6085\n",
      "Epoch 3 Batch 100 Loss 1.5140\n",
      "Epoch 3 Batch 150 Loss 1.4993\n",
      "\n",
      "Epoch 3 Loss: 1.5395\n",
      "Time taken for 1 epoch 11.73 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.5284\n",
      "Epoch 4 Batch 50 Loss 1.4457\n",
      "Epoch 4 Batch 100 Loss 1.4078\n",
      "Epoch 4 Batch 150 Loss 1.4155\n",
      "\n",
      "Epoch 4 Loss: 1.4435\n",
      "Time taken for 1 epoch 12.00 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3669\n",
      "Epoch 5 Batch 50 Loss 1.4092\n",
      "Epoch 5 Batch 100 Loss 1.3724\n",
      "Epoch 5 Batch 150 Loss 1.3099\n",
      "\n",
      "Epoch 5 Loss: 1.3764\n",
      "Time taken for 1 epoch 12.43 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.2922\n",
      "Epoch 6 Batch 50 Loss 1.3351\n",
      "Epoch 6 Batch 100 Loss 1.3114\n",
      "Epoch 6 Batch 150 Loss 1.3394\n",
      "\n",
      "Epoch 6 Loss: 1.3245\n",
      "Time taken for 1 epoch 12.29 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.3061\n",
      "Epoch 7 Batch 50 Loss 1.2585\n",
      "Epoch 7 Batch 100 Loss 1.2278\n",
      "Epoch 7 Batch 150 Loss 1.2498\n",
      "\n",
      "Epoch 7 Loss: 1.2795\n",
      "Time taken for 1 epoch 12.27 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2141\n",
      "Epoch 8 Batch 50 Loss 1.2318\n",
      "Epoch 8 Batch 100 Loss 1.2517\n",
      "Epoch 8 Batch 150 Loss 1.2478\n",
      "\n",
      "Epoch 8 Loss: 1.2390\n",
      "Time taken for 1 epoch 12.27 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1658\n",
      "Epoch 9 Batch 50 Loss 1.2229\n",
      "Epoch 9 Batch 100 Loss 1.2179\n",
      "Epoch 9 Batch 150 Loss 1.2223\n",
      "\n",
      "Epoch 9 Loss: 1.1986\n",
      "Time taken for 1 epoch 12.29 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1662\n",
      "Epoch 10 Batch 50 Loss 1.1513\n",
      "Epoch 10 Batch 100 Loss 1.1334\n",
      "Epoch 10 Batch 150 Loss 1.1517\n",
      "\n",
      "Epoch 10 Loss: 1.1583\n",
      "Time taken for 1 epoch 12.52 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539497f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
